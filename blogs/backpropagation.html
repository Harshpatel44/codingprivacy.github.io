---
layout: "blog_layout"
web_title: "Backpropagation - A pillar of neural networks"
page_title: "Backpropagation - A pillar of neural networks"
upload-date: "April 21, 2020"
author: "Harsh Patel"
---


<p>Hello people, this blog is about the backpropagation algorithm used in Machine learning. It is one of the most important algorithms and can be described as the pillar of the neural networks. I tried to make it as simple as possible, so I am sure you would be able to understand the fundamental working of backpropagation after reading this post.</p>

<h2>Overview</h2>
<p>As we have seen in the last post that during training, Neural networks undergo feedforward and backpropagation mechanism. Data moves from input to output i.e. first layer to the last layer in feedforward mechanism and then we start backward and move from right to left to again first layer in backpropagation. The main purpose of backpropagation is that we update other parameters of the network with a change in a loss. </p>

<h2>Introduction</h2>
<p>From input data, we generate predicted data using feedforward propagation. then we find a loss between predicted data and true data. Now as loss gets updated, we need to update all network parameters before the next iteration. So, using backpropagation, we update all the values for the change in a loss. We use chain rule in derivation to calculate this.
</p>

<h2>Feedforward at a glance</h2>
<p>Notations: </p>
    <div style="text-align-last: start;">
        <h4>
        $$ T = \text{true data} $$
        $$ L = loss $$
        $$ Y = \text{predicted data} $$
        $$ W = \text{weights ( }W_1 \text{ means layer 1 weights) } $$
        $$X = \text{input data} $$
        $$B = bias $$  
        </h4>
    </div>

    
    <h4>$$ Z_1 = W_1 \cdot X + B_1 $$</h4>
    <h4>$$ H_1 = \sigma(Z_1) $$</h4>
    <h4>$$ Z_2 = W_2 \cdot H_1 + B_2 $$</h4>
    <h4>$$ Y = \sigma(Z_2) $$</h4>
    <h4>$$ L = \frac{1}{N} \sum_{i=0}^{N} (Y-T)^{2}  $$</h4>
    <br>

    <center>
         <img src="{{ site.url }}/img/backpropagation/feedforward2.PNG" class="img-responsive" height="70%" width="70%">
        <caption>figure 1. Computation graph for single layer network</caption>
    </center>
    <br>

<p>The above computation graph shows single hidden layer working of the network. If you are not able to understand it, you can know more about the feedforward network on my previous blog.  <b  style="color: #1abc9c;"><a href="../../blogs/feedforward-propagation">click here</a></b>.</p>
        
<h2>Backpropagation</h2>
<p>
Backpropagation is calculated for all the hidden layers of the network, so we will calculate for two layers according to above network.
Now Let’s look at last layer backpropagation on the network. As we have discussed before, we have to find the change of other values with change in loss.<br>
</p>
    <center>
         <img src="{{ site.url }}/img/backpropagation/first_layer.PNG" class="img-responsive" height="70%" width="70%">
        <caption>figure 1. Computation graph for single layer network</caption>
    </center>
<h4>
<p>Last layer backpropagation means from \(L\) to \( W_2 , B2\) and \(Z2\) as shown in the above figure. So the values we need to find are:</p>
</h4>
<div style="text-align-last: start">
    <h4>
        $$    \frac{\partial L}{\partial Y} = \text {change of Y with loss}  $$
        $$    \frac{\partial Y}{\partial Z_2} = \text {change of Z with Y}     $$
        $$    \frac{\partial Z_2}{\partial W_2} = \text {change of W with Z}     $$
        $$    \frac{\partial Z_2}{\partial B_2} = \text {change of B with Z}     $$
    </h4>
</div>
<h4>
$$ 
\frac{\partial L}{\partial Y} , 
 \frac{\partial Y}{\partial Z_2} , 
 \frac{\partial Z_2}{\partial W_2} ,
 \frac{\partial Z_2}{\partial B_2}
$$
</h4>
<p>Of course, we can find all these values, but here is a question, what values can we update after computing? We cannot update Y as it is predicted value, which is generated by the model, nor can we update the Z value. We can change bias and weights as that is provided by us from the start. Bias and weights do computation with our input data throughout the layers and decide how many features should be propagated further. We can also not change input data as changing it will not be a good idea ever (we want to find a solution to the problem, not change the problem). So we are going to update these values.
</p>

<h4>
$$ \frac{\partial Z_2}{\partial W_2} , \frac{\partial Z_2}{\partial B_2} $$ 
</h4>

<p>
To compute this, we will apply the chain rule.
Chain rule works as follows:
</p>

<h4>
$$ \frac{\partial a}{\partial d} = \frac{\partial a}{\partial b} \times \frac{\partial b}{\partial c} \times \frac{\partial c}{\partial d} $$ </h4>

<p>Hence applying it in our scenario: </p>
<h4>
$$    \frac{\partial L}{\partial W_2} = \frac{\partial L}{\partial Y} \times \frac{\partial Y}{\partial Z_2} \times \frac{\partial Z_2}{\partial W_2} \quad,\quad \frac{\partial L}{\partial B_2} = \frac{\partial L}{\partial Y} \times \frac{\partial Y}{\partial Z_2} \times \frac{\partial Z_2}{\partial B_2}$$
</h4>

<h3>An example</h3>
<p>
This is like passing a message to each node backward. Let us take a small example to understand this better. Suppose you are going on a trip to Las Vegas with a budget of 30K. Now you want to know how much money you will be left with for using in casino eliminating other expenses. You have an idea that 10% will be travel cost and 20% will be accommodation cost. 
Let us compute this problem and help you out with it. <br>
Budget = 30K <br>
travel cost = 30K * 0.10 = 3K <br>
accommodation cost = 30K * 0.20 = 6K <br>
Money after eliminating costs = 21K <br>
Hence you will have 21K to use In the casino. Now if you change the budget, you would have to compute all the costs again to calculate remaining money. <br><br>
Same way as you find loss, you need to change all the intermediate values which finally gives you the changes in weights and bias. This would have given you an idea of why and what happens.
</p>

<h3>Continuing further</h3>
<p> Now let us calculate chain rule derivatives.<br>
</p>

<h4>
\( \text{Sigmoid as an activation function } \sigma = \frac{e^x}{(1 + e^{-x})} \)<br><br>
\( \text{derivative of sigmoid } \sigma = \sigma' = \sigma(x) \times (1 - \sigma(x)) \)<br><br>
\( \text{MSE loss } = \frac{1}{N} \sum_{i=0}^{N} (y-t)^{2} \) <br><br><br>
</h4>

<center>
         <img src="{{ site.url }}/img/backpropagation/table.PNG" class="img-responsive" height="50%" width="50%">
         <caption>table 1. Calculation of all derivatives</caption>
</center>
<br>
<p>Using table 1. we get the following values. </p>
<h4>
$$ \frac{\partial L}{\partial W} = (Y - T) \times \sigma'(Z) \times X $$
$$ \text{As } \frac{\partial Z}{\partial B} = 1, \quad \frac{\partial L}{\partial B} = (Y - T) \times \sigma'(Z) \quad \text{i.e. equal to }\ \frac{\partial L}{\partial Z} $$
</h4>
    
<p>Hence as we computed the derivatives for last hidden layer, we will now move further and compute for the second last layer of neural network (i.e first layer of our network) and then generate a formula for the arbitrary number of layers.</p>
<p>
Till now, we have computed <br>
$$ \frac{\partial L}{\partial Z_1} = (Y - T) \times \sigma'(Z_1) $$
$$ \frac{\partial L}{\partial W_1} = (Y - T) \times \sigma'(Z1) \times X $$
DL/DW1 =  (y - t) * σ'(z1) * X
Applying Chainrule for \(2^{nd} \) layer

DL/DW2 = DL/DY1 * DY1/DZ1 * DZ1/DH1 * DH1/DZ2 * DZ2/DW2
</p>